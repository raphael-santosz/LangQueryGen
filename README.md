# LangQueryGen

LangQueryGen is a modular AI-powered backend that transforms natural language questions into SQL queries using **Retrieval-Augmented Generation (RAG)** with **LangChain**, **Ollama** (local LLM like Mistral 7B), and **Flask**. The system also includes semantic validation of user input and a Node.js layer for interaction or interface.

---

## ğŸš€ Features

- ğŸ” **Natural Language to SQL** with LangChain and a local model (via Ollama)
- ğŸ§  **RAG pipeline** with FAISS and HuggingFace embeddings
- âœ… **Input validation agent** for filtering sensitive questions
- ğŸ› ï¸ **Modular Flask API** structure
- ğŸ§© **Node.js Integration** for frontend or orchestration
- ğŸ”’ **Fully local and secure** â€“ no external API calls

---

## ğŸ“ Project Structure

```
flask-server/
â”œâ”€â”€ app.py                 # Flask entry point
â”œâ”€â”€ routes/                # API routes (Blueprint)
â”œâ”€â”€ services/              # LangChain agents and logic
â”œâ”€â”€ models/                # Pydantic request/response models
â”œâ”€â”€ utils/                 # Auxiliary functions (e.g., query fixer)
â”œâ”€â”€ venv/                  # Python virtual environment
â”œâ”€â”€ package-lock.json      # Node.js dependencies
â””â”€â”€ ...
```

---

## ğŸ§‘â€ğŸ’» How to Run (Backend)

### 1. Clone the repo and create the virtual environment

```bash
git clone https://github.com/raphael-santosz/LangQueryGen.git
cd LangQueryGen/flask-server
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
pip install -r requirements.txt
python app.py
```

---

## ğŸ§‘â€ğŸ’» How to Run (Frontend)

```bash
cd client
npm install
npm run dev
```

---

## ğŸ“¦ Requirements

- Python 3.10+
- Node.js 18+
- Ollama installed locally
- SQL Server or compatible DB
- Optional: FAISS, LangChain, HuggingFace embeddings

---

## âš™ï¸ Technologies

- ğŸ Flask + Pydantic
- ğŸ§  LangChain + Ollama + HuggingFace
- ğŸ§® SQLAlchemy
- ğŸŒ Node.js (for UI or extended API)
- ğŸ§  Mistral 7B / LLaMA3

---

## ğŸ›¡ï¸ Security

The validation agent checks whether the user input relates to sensitive topics like **salary** or **payment**, returning `"Bloqueado"` if found.  
All LLM interactions are **local and secure**, ensuring full data privacy.

---

## âœ¨ Authors

Developed by **Raphael Augusto Santos**  and **Rafael Azzolini**
[GitHub](https://github.com/raphael-santosz)
